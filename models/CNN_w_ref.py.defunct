from torch import nn
import torch

from remora.activations import swish
from remora import constants


class network(nn.Module):
    _variable_width_possible = False

    def __init__(
        self,
        size=constants.DEFAULT_SIZE,
        kmer_len=constants.DEFAULT_KMER_LEN,
        num_out=2,
    ):
        super().__init__()
        self.sig_conv1 = nn.Conv1d(1, size, 3)
        self.seq_conv1 = nn.Conv2d(1, size, (kmer_len * 4, 3))
        self.merge_conv1 = nn.Conv1d(size * 2, size, 5)

        self.fc1 = nn.Linear(size * (kmer_len - 1), num_out)

        self.dropout = nn.Dropout(p=0.3)

    def forward(self, sigs, seqs):
        sigs = sigs.permute(1, 2, 0)
        sigs = self.dropout(swish(self.sig_conv1(sigs)))

        seqs = seqs.permute(1, 2, 0)
        seqs = self.dropout(swish(self.seq_conv1(seqs.unsqueeze(1))))
        seqs = seqs.squeeze(2)
        z = torch.cat((sigs, seqs), 1)
        z = self.dropout(swish(self.merge_conv1(z)))
        z = z.squeeze(2)
        z = torch.flatten(z, start_dim=1)
        z = torch.softmax(self.fc1(z), dim=1)

        return z
